{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be making a neural network to solve a handwritten digit recognition problem from the MNIST data set.  No deep learning libraries such as Tensorflow and Pytorch will be used here. However, NumPy will be imported to increase efficiency and simulate the inner workings of deep learning libraries. Pandas and Matplotlib will also be used to help handle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read in the training data and get familiar with it. The training set here has 1 column describing the label (which digit the picture represents) followed by columns describing the color value of the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7   \n",
       "0      1       0       0       0       0       0       0       0       0  \\\n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779   \n",
       "0       0  ...         0         0         0         0         0         0  \\\n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the data into training and cross-validation (CV) sets. There are 42,000 examples so we will pick 10,000 random ones to use for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data before splitting: (42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(train_data)\n",
    "m, n = data.shape\n",
    "print(f\"Shape of data before splitting: {m, n}\")\n",
    "\n",
    "# Shuffle the data first\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# CV set\n",
    "cv = data[:10000].T\n",
    "X_cv = cv[1:n] / 255\n",
    "y_cv = cv[0]\n",
    "\n",
    "# Training set\n",
    "train = data[10000:m].T\n",
    "X_train = train[1:n] / 255\n",
    "y_train = train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's develop the Neural Network class we will be working with. This neural network will have 3 layers not including the input layer so I can get some practice developing multi-layered neural networks. The first layer will have 20 neurons, the second will have 15 neurons, and the last (output layer) will have 10 neurons corresponding to the 10 classes of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the neural network with random weights (w1, b1 , w2, b2, w3, b3)\n",
    "        '''\n",
    "\n",
    "        self.W1 = np.random.rand(20, 784) - 0.5\n",
    "        self.b1 = np.random.rand(20, 1) - 0.5\n",
    "        self.W2 = np.random.rand(15, 20) - 0.5\n",
    "        self.b2 = np.random.rand(15, 1) - 0.5\n",
    "        self.W3 = np.random.rand(10, 15) - 0.5\n",
    "        self.b3 = np.random.rand(10, 1) - 0.5\n",
    "    \n",
    "    def reLU(self, Z):\n",
    "        '''\n",
    "        Defines reLU activation function\n",
    "\n",
    "        Args:\n",
    "            Z (np.ndarray): Wn * An-1 + bn\n",
    "        \n",
    "        Returns:\n",
    "            reLU activation function applied to Z\n",
    "        '''\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def reLU_derivative(self, Z):\n",
    "        '''\n",
    "        Returns the derivative of reLU function\n",
    "\n",
    "        Args:\n",
    "            Z (np.ndarray): Wn * An-1 + bn\n",
    "        \n",
    "        Returns:\n",
    "            derivative of Z (1 or 0)\n",
    "        '''\n",
    "\n",
    "        return Z > 0\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        '''\n",
    "        Defines softmax activation function\n",
    "\n",
    "        Args:\n",
    "            Z (np.ndarray): Wn * An-1 + bn\n",
    "\n",
    "        Returns:\n",
    "            ndarray Z with softmax applied \n",
    "        '''\n",
    "\n",
    "        return np.exp(Z) / sum(np.exp(Z))\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        '''\n",
    "        Carry out forward propagation in neural network layers\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Training data\n",
    "        \n",
    "        Returns:\n",
    "            6-tuple containing intermediate terms (Z1, A1, Z2, A2, Z3, A3)\n",
    "        '''\n",
    "\n",
    "        Z1 = np.dot(self.W1, X) + self.b1\n",
    "        A1 = self.reLU(Z1)\n",
    "        Z2 = np.dot(self.W2, A1) + self.b2\n",
    "        A2 = self.reLU(Z2)\n",
    "        Z3 = np.dot(self.W3, A2) + self.b3\n",
    "        A3 = self.softmax(Z3)\n",
    "\n",
    "        return Z1, A1, Z2, A2, Z3, A3\n",
    "    \n",
    "    def one_hot(self, Y):\n",
    "        '''\n",
    "        Uses one-hot encoding to encode each label\n",
    "         \n",
    "        Args:\n",
    "            Y (np.ndarray): labels for data\n",
    "        \n",
    "        Returns:\n",
    "            one_hot (np.ndarray): one-hot encoded ndarray for labels of Y\n",
    "        '''\n",
    "\n",
    "        # Use max(Y) + 1 for # of columns because thats how many digits there are (0-9)\n",
    "        one_hot = np.zeros((Y.size, max(Y) + 1))\n",
    "        # For each row, apply one-hot encoding at the specified Y_row = Y column and turn it from 0 to 1\n",
    "        # Note: can do this iteratively, but numpy vectorizes this for us to make our NN a little more efficient\n",
    "        one_hot[np.arange(Y.size), Y] = 1\n",
    "        return one_hot.T\n",
    "    \n",
    "    # Note: some parameters of back prop are not used, but are included as parameters\n",
    "    # because it's a little easier to use since it follows the pattern Z, A, Z, A, Z, A, X, Y\n",
    "    # Slowdown because of this addition is negligible\n",
    "    def back_prop(self, Z1, A1, Z2, A2, Z3, A3, X, Y):\n",
    "        '''\n",
    "        Carry out backward propagation in neural netowrk layers\n",
    "\n",
    "        Args:\n",
    "            Z1 (np.ndarray): W1 * A0 + b1\n",
    "            A1 (np.ndarray): reLU(Z1)\n",
    "            Z2 (np.ndarray): W2 * A1 + b2\n",
    "            A2 (np.ndarray): reLU(Z2)\n",
    "            Z3 (np.ndarray): W3 * A2 + b3\n",
    "            A3 (np.ndarray): softmax(Z3)\n",
    "            X (np.ndarray): data\n",
    "            Y (np.ndarray): labels\n",
    "        \n",
    "        Returns:\n",
    "            6-tuple containing gradients (dW1, db1, dW2, db2, dW3, db3)\n",
    "        '''\n",
    "        \n",
    "        # Output layer\n",
    "        one_hot_y = self.one_hot(Y)\n",
    "        dZ3 = A3 - one_hot_y\n",
    "        dW3 = 1 / m * np.dot(dZ3, A2.T)\n",
    "        db3 = 1 / m * np.sum(dZ3)\n",
    "\n",
    "        # 2nd layer\n",
    "        dZ2 = np.dot(self.W3.T, dZ3) * self.reLU_derivative(Z2)\n",
    "        dW2 = 1 / m * np.dot(dZ2, A1.T)\n",
    "        db2 = 1 / m * np.sum(dZ2)\n",
    "\n",
    "        # 1st layer\n",
    "        dZ1 = np.dot(self.W2.T, dZ2) * self.reLU_derivative(Z1)\n",
    "        dW1 = 1 / m * np.dot(dZ1, X.T)\n",
    "        db1 = 1 / m * np.sum(dZ1)\n",
    "\n",
    "        return dW3, db3, dW2, db2, dW1, db1\n",
    "\n",
    "    def update(self, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
    "        '''\n",
    "        Updates weights and biases for gradient descent\n",
    "\n",
    "        Args:\n",
    "            dW1 (float32): gradient for layer 1 weight\n",
    "            db1 (float32): gradient for layer 1 bias\n",
    "            dW2 (float32): gradient for layer 2 weight\n",
    "            db2 (float32): gradient for layer 2 bias\n",
    "            dW3 (float32): gradient for layer 3 weight\n",
    "            db3 (float32): gradient for layer 3 bias\n",
    "            learning_rate (float32): learning rate alpha for gradient descent\n",
    "        '''\n",
    "\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n",
    "    \n",
    "    def gradient_descent(self, X_train, y_train, learning_rate, iters):\n",
    "        '''\n",
    "        Runs gradient descent to train the neural network\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): training data\n",
    "            y (np.ndarray): training labels\n",
    "            learning_rate (float32): learning rate alpha\n",
    "            iters (int32): number of iterations to run gradient descent\n",
    "        \n",
    "        Returns:\n",
    "            6-tuple containing trained weight and bias terms (W1, b1, W2, b2, W3, b3)\n",
    "        '''\n",
    "        \n",
    "        for i in range(iters):\n",
    "            Z1, A1, Z2, A2, Z3, A3 = self.forward_prop(X_train)\n",
    "            dW3, db3, dW2, db2, dW1, db1 = self.back_prop(Z1, A1, Z2, A2, Z3, A3, X_train, y_train)\n",
    "            self.update(dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "\n",
    "            # Print status report every 100 iterations\n",
    "            if i % 100 == 0 or i == 0:\n",
    "                print(f\"Iteration: {i}\")\n",
    "                pred = np.argmax(A3, 0)\n",
    "                print(f\"Accuracy: {np.sum(pred == y_train) / y_train.size}\")\n",
    "        \n",
    "        return self.W1, self.b1, self.W2, self.b2, self.W3, self.b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot. Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Accuracy: 0.08871875\n",
      "Iteration: 100\n",
      "Accuracy: 0.4455625\n",
      "Iteration: 200\n",
      "Accuracy: 0.62915625\n",
      "Iteration: 300\n",
      "Accuracy: 0.7061875\n",
      "Iteration: 400\n",
      "Accuracy: 0.74515625\n",
      "Iteration: 500\n",
      "Accuracy: 0.77334375\n",
      "Iteration: 600\n",
      "Accuracy: 0.7943125\n",
      "Iteration: 700\n",
      "Accuracy: 0.8111875\n",
      "Iteration: 800\n",
      "Accuracy: 0.82609375\n",
      "Iteration: 900\n",
      "Accuracy: 0.83653125\n",
      "Iteration: 1000\n",
      "Accuracy: 0.845\n",
      "Final weights and bias terms: (W1, b1) = (array([[ 0.35513463,  0.02991467, -0.38423207, ..., -0.20351427,\n",
      "         0.03557606,  0.19541762],\n",
      "       [ 0.24591633,  0.42376314,  0.29125398, ...,  0.34085754,\n",
      "        -0.08389955, -0.33118801],\n",
      "       [-0.03092499,  0.17866506, -0.08346867, ...,  0.43955289,\n",
      "        -0.28426749,  0.10444861],\n",
      "       ...,\n",
      "       [ 0.25516906,  0.27780857,  0.13788438, ...,  0.10522659,\n",
      "        -0.24858966,  0.37746945],\n",
      "       [ 0.22764931,  0.05213144,  0.28858393, ...,  0.25644825,\n",
      "        -0.3948064 ,  0.20437116],\n",
      "       [-0.30596184,  0.40019861, -0.27385705, ...,  0.38769157,\n",
      "         0.07191846,  0.11493995]]), array([[0.17046201],\n",
      "       [0.22630342],\n",
      "       [0.0628633 ],\n",
      "       [0.48094937],\n",
      "       [1.0037665 ],\n",
      "       [0.55149019],\n",
      "       [0.30192051],\n",
      "       [0.63359051],\n",
      "       [0.45930443],\n",
      "       [0.11118788],\n",
      "       [0.78690224],\n",
      "       [0.52436051],\n",
      "       [0.53278711],\n",
      "       [0.53252109],\n",
      "       [0.26409691],\n",
      "       [1.01753609],\n",
      "       [0.84510615],\n",
      "       [0.83880789],\n",
      "       [0.22207836],\n",
      "       [0.47215699]])), (W2, b2) = (array([[ 1.96944274e-01,  4.16984192e-01,  8.50095815e-02,\n",
      "        -1.92635873e-01, -2.52944270e-02, -1.03793062e-01,\n",
      "        -7.57637617e-03,  4.10448047e-01, -1.59373481e-01,\n",
      "        -3.67079123e-01, -1.66417691e-01,  3.26627072e-01,\n",
      "        -4.62955630e-01, -9.86261827e-02, -2.54671545e-02,\n",
      "        -3.57842742e-01, -4.58467516e-01,  2.45257801e-01,\n",
      "         2.19509274e-01,  2.92696200e-01],\n",
      "       [ 3.05744021e-01, -9.82749260e-02,  2.07553328e-01,\n",
      "        -4.27708508e-01, -4.21665457e-02, -1.36822208e-01,\n",
      "        -1.16139782e-01,  2.79626239e-02,  6.91153463e-01,\n",
      "         6.18427714e-02, -1.00286997e-01,  2.70659599e-01,\n",
      "         4.86399151e-04, -6.02800893e-02,  1.10615405e-01,\n",
      "         5.23431613e-01,  2.36185663e-01, -5.96624134e-03,\n",
      "         4.29801579e-01, -3.71746443e-01],\n",
      "       [-1.18486207e-01, -1.32452343e-01, -1.06331241e-01,\n",
      "         6.17312068e-01, -1.21691901e-01, -9.37873242e-02,\n",
      "        -3.25608505e-01,  6.13802253e-01,  4.71404540e-01,\n",
      "        -2.74704536e-02,  3.84832099e-02,  2.32388958e-01,\n",
      "        -3.02945085e-01,  1.24352521e-01,  8.86907538e-02,\n",
      "        -2.24783983e-01, -5.27633569e-01, -2.71518595e-01,\n",
      "         2.07406093e-01,  4.57630512e-01],\n",
      "       [ 1.38529336e-01, -1.26948255e-01, -4.29090541e-01,\n",
      "        -4.79713925e-01,  2.84733337e-01, -1.98371045e-01,\n",
      "         3.31048247e-01, -1.45978790e-01, -2.24081281e-01,\n",
      "        -3.92280627e-01, -1.99848351e-02, -2.21322544e-01,\n",
      "         1.08109853e-02,  4.20553305e-01,  1.12461518e-01,\n",
      "         2.47980615e-01, -8.10751747e-02,  8.96144129e-02,\n",
      "        -1.11240821e-01,  1.03941421e-01],\n",
      "       [ 2.70137493e-01, -4.19806656e-01,  9.21964767e-02,\n",
      "         3.22336154e-01,  4.43820157e-02, -4.83400843e-01,\n",
      "        -3.42225606e-01, -5.11522256e-01,  2.55792544e-01,\n",
      "         3.66762183e-01, -3.63982254e-01, -2.87510566e-01,\n",
      "        -7.93589521e-02, -1.88004981e-01, -4.16270173e-01,\n",
      "        -2.99450070e-01,  2.76502115e-01,  8.49485671e-02,\n",
      "         4.51786059e-02, -4.00095242e-03],\n",
      "       [ 5.40411683e-01, -2.25146951e-01,  3.63376747e-01,\n",
      "        -1.31392956e-01,  3.52590913e-01, -2.74772233e-01,\n",
      "         3.63803674e-02, -4.85520527e-02,  9.39375942e-02,\n",
      "         4.99247478e-01,  1.63112019e-01, -3.63476196e-01,\n",
      "        -1.04202945e-01,  9.36735484e-02,  9.72107120e-02,\n",
      "        -5.00872969e-01, -2.14054969e-01, -3.09120989e-01,\n",
      "        -9.76793088e-02, -1.70047598e-01],\n",
      "       [ 2.76870320e-01, -3.39991259e-01, -2.39714441e-01,\n",
      "         1.40237401e-01, -1.68186939e-01, -6.22153940e-02,\n",
      "        -1.76512755e-01, -2.98267240e-01, -1.57081600e-01,\n",
      "         2.88438261e-01,  4.61984587e-01, -3.80230956e-01,\n",
      "         3.07465349e-01, -3.11317639e-02,  7.16073200e-02,\n",
      "         2.25925048e-01,  4.01328515e-02, -3.05574193e-01,\n",
      "        -2.48116853e-01,  4.61475192e-01],\n",
      "       [-2.74669203e-01,  1.94366717e-01, -1.35796667e-01,\n",
      "        -3.07722883e-01, -4.95875587e-03,  3.61703329e-01,\n",
      "        -1.81145972e-01, -5.34622579e-02, -3.78599210e-01,\n",
      "        -3.84633026e-01,  4.74767299e-01, -3.27569060e-01,\n",
      "         3.64003001e-03, -4.51665225e-01, -9.05237911e-02,\n",
      "         4.67009289e-01, -1.72874726e-01, -2.45657775e-01,\n",
      "        -8.16704543e-02, -4.62404083e-01],\n",
      "       [-4.24230964e-01,  2.13220220e-01, -7.25133950e-02,\n",
      "         2.85822660e-01,  4.08926375e-01, -4.77980401e-01,\n",
      "         1.23747818e-01,  8.58095630e-02, -9.07453912e-02,\n",
      "         5.19190374e-01,  1.07157614e-01,  4.30428365e-01,\n",
      "         1.76841826e-01, -7.63096305e-02,  7.61016317e-02,\n",
      "         5.84968258e-02,  3.85336515e-01,  1.84315940e-01,\n",
      "         6.24715479e-02, -2.05343572e-01],\n",
      "       [ 7.57176239e-01,  3.83433419e-01, -2.16560352e-01,\n",
      "         5.56610180e-02, -2.42485088e-01, -2.55440668e-01,\n",
      "         2.87955565e-01, -9.05380488e-02,  4.98506238e-01,\n",
      "        -1.95714970e-01, -2.52732456e-01,  4.34227577e-01,\n",
      "        -2.17927855e-02,  3.43331231e-02,  7.19545124e-02,\n",
      "         3.04878214e-01,  4.38930565e-01,  2.59000235e-03,\n",
      "        -5.66845423e-03, -4.04919476e-01],\n",
      "       [-4.38041177e-01,  2.41314954e-01,  4.46545074e-01,\n",
      "         2.94076366e-01, -4.66033728e-01,  2.79393370e-01,\n",
      "        -2.02563326e-01, -4.64289038e-01, -3.54012204e-01,\n",
      "        -4.14177722e-01, -3.56706333e-01,  4.11581267e-01,\n",
      "         1.80194197e-01,  3.70261794e-01, -4.02551027e-01,\n",
      "        -2.61803161e-04, -4.85755610e-01, -4.65214911e-01,\n",
      "         4.95166907e-02, -4.48023807e-01],\n",
      "       [ 1.35956009e-01, -2.30064983e-01, -1.91656692e-02,\n",
      "        -1.30399954e-02, -3.59353468e-01,  1.80527575e-01,\n",
      "        -3.83352493e-01,  4.71740193e-02,  2.88449372e-01,\n",
      "         1.69580506e-02,  1.64314387e-01,  1.05821570e-02,\n",
      "         7.49883519e-01,  2.34126528e-01, -1.07384459e-01,\n",
      "        -4.62798625e-01, -3.52290367e-02, -3.57553311e-01,\n",
      "        -1.67428484e-01,  3.84536625e-01],\n",
      "       [-6.93363471e-03,  1.19126789e-02, -7.27426601e-03,\n",
      "        -4.59247729e-02,  4.72617945e-01,  3.26878796e-01,\n",
      "        -1.46611298e-02,  2.05389665e-01, -1.13851820e-02,\n",
      "        -3.54287335e-01, -4.19605903e-02, -4.00514926e-02,\n",
      "         5.23713967e-01,  1.11484489e-01,  1.90634480e-01,\n",
      "         5.95996020e-01, -1.71084093e-01, -5.51983933e-01,\n",
      "        -1.78451100e-01,  2.70477916e-01],\n",
      "       [ 3.07553109e-01, -2.98798441e-01, -3.70946450e-01,\n",
      "        -1.09437855e-01, -1.63634376e-01,  3.03440573e-01,\n",
      "         2.36417888e-01,  3.43860263e-01,  1.75515651e-01,\n",
      "        -2.19162348e-01,  1.93619494e-01,  7.55382987e-02,\n",
      "         1.98246179e-01, -2.79754929e-01, -8.66877609e-02,\n",
      "        -1.96368258e-01,  4.51815525e-01, -4.24032107e-01,\n",
      "        -1.03310114e-01,  1.91323346e-01],\n",
      "       [ 6.05553688e-01, -3.19377041e-01, -2.13580613e-01,\n",
      "        -6.04176087e-01,  4.80281167e-01,  2.30070805e-01,\n",
      "         2.28847265e-01,  8.81098521e-01, -1.84363342e-01,\n",
      "         1.71896021e-01, -2.06935029e-01, -7.62395410e-03,\n",
      "        -2.98001299e-01,  1.24064420e-01, -2.36152344e-01,\n",
      "        -4.44204017e-01,  5.11259155e-01,  3.10454852e-01,\n",
      "         2.60827334e-01, -2.92887775e-01]]), array([[-0.28346841],\n",
      "       [-0.31982942],\n",
      "       [ 0.55995478],\n",
      "       [-0.40616754],\n",
      "       [ 0.38314931],\n",
      "       [ 0.15567926],\n",
      "       [ 0.19957835],\n",
      "       [-0.40319621],\n",
      "       [ 0.3449322 ],\n",
      "       [ 0.46135661],\n",
      "       [ 0.17032235],\n",
      "       [-0.35487341],\n",
      "       [ 0.13567554],\n",
      "       [ 0.31747942],\n",
      "       [-0.38248258]])), (W3, b3) = (array([[-0.47110362,  0.18389399,  0.09448645, -0.49976761,  0.36907959,\n",
      "         0.18204969,  0.38538955, -0.19970358,  0.70163738, -0.54544198,\n",
      "         0.09977857,  0.38041349, -0.33798777, -0.30990531, -0.10811247],\n",
      "       [ 0.3055112 ,  0.65204562, -0.09183311, -0.00190763, -0.49330203,\n",
      "        -0.10565836, -0.31244529, -0.27771548, -0.48373301,  0.35595468,\n",
      "         0.29313004, -0.35751162, -0.1932529 , -0.21300056,  0.38480562],\n",
      "       [ 0.19651901,  0.5799309 ,  0.55871816, -0.22531564,  0.22142718,\n",
      "         0.41567983, -0.16523709,  0.01249705, -0.01817127, -0.32097917,\n",
      "         0.1569751 , -0.28195645, -0.12174795, -0.29418166,  0.03805234],\n",
      "       [-0.352214  ,  0.00906365,  0.02941391,  0.12639017,  0.45016997,\n",
      "         0.60589827,  0.15721666,  0.42105573, -0.21776032, -0.13620187,\n",
      "         0.37097828, -0.0425901 , -0.09433233,  0.18068385,  0.41387993],\n",
      "       [ 0.0264866 , -0.1719303 ,  0.29800004,  0.36843263, -0.54010638,\n",
      "        -0.26906625,  0.26181252,  0.29103453,  0.24574525, -0.17933367,\n",
      "        -0.24871449, -0.13931864,  0.6082222 ,  0.54303563,  0.20671213],\n",
      "       [ 0.41722372,  0.07347933, -0.2467059 , -0.26755267, -0.08764397,\n",
      "         0.32817936,  0.31970548,  0.30399597,  0.27472626,  0.31492066,\n",
      "         0.45357752,  0.3643429 ,  0.30032028,  0.1067321 , -0.17996681],\n",
      "       [ 0.08875799,  0.09643233,  0.63570064, -0.05118513, -0.1706587 ,\n",
      "        -0.45171617,  0.06491446,  0.47846242,  0.0252677 ,  0.2728628 ,\n",
      "        -0.06706509,  0.39394199, -0.1111644 ,  0.20160814, -0.52501179],\n",
      "       [ 0.41963174, -0.11964225,  0.10482132, -0.43469827, -0.37532769,\n",
      "        -0.17020899,  0.30592573, -0.35637472,  0.61291632, -0.57875702,\n",
      "        -0.0330239 , -0.16746802, -0.55525889,  0.18360214,  0.84629783],\n",
      "       [-0.08624331,  0.30299394, -0.04265485,  0.25233565, -0.13492186,\n",
      "         0.61234479, -0.59315247,  0.3804402 , -0.18126928, -0.15706874,\n",
      "        -0.04167423,  0.54536231,  0.19204124,  0.24702455,  0.14460064],\n",
      "       [-0.2334809 , -0.52187588, -0.05082365, -0.34927784, -0.18417143,\n",
      "         0.13112503, -0.45996549, -0.32285461,  0.16806029, -0.42314742,\n",
      "        -0.38852648,  0.29808199,  0.47917504,  0.48021076,  0.63171574]]), array([[-0.02544689],\n",
      "       [-0.42499362],\n",
      "       [ 0.27328023],\n",
      "       [ 0.44354665],\n",
      "       [ 0.04332468],\n",
      "       [ 0.4139975 ],\n",
      "       [ 0.01395297],\n",
      "       [ 0.30640994],\n",
      "       [-0.18185304],\n",
      "       [-0.08497017]]))\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "W1, b1, W2, b2, W3, b3 = nn.gradient_descent(X_train, y_train, 0.05, 1000)\n",
    "print(f\"Final weights and bias terms on training data: (W1, b1) = {W1, b1}, (W2, b2) = {W2, b2}, (W3, b3) = {W3, b3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try it on the CV data to make sure we aren't overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Accuracy: 0.1042\n",
      "Iteration: 100\n",
      "Accuracy: 0.2054\n",
      "Iteration: 200\n",
      "Accuracy: 0.2846\n",
      "Iteration: 300\n",
      "Accuracy: 0.3617\n",
      "Iteration: 400\n",
      "Accuracy: 0.4248\n",
      "Iteration: 500\n",
      "Accuracy: 0.4847\n",
      "Iteration: 600\n",
      "Accuracy: 0.5513\n",
      "Iteration: 700\n",
      "Accuracy: 0.615\n",
      "Iteration: 800\n",
      "Accuracy: 0.6538\n",
      "Iteration: 900\n",
      "Accuracy: 0.6871\n",
      "Iteration: 1000\n",
      "Accuracy: 0.7107\n",
      "Final weights and bias terms on cross-validation data: (W1, b1) = (array([[-0.24105684, -0.0646549 , -0.11179161, ..., -0.02761542,\n",
      "        -0.3497677 , -0.27438858],\n",
      "       [ 0.07011422,  0.25973125,  0.45540723, ..., -0.27892262,\n",
      "        -0.48534694,  0.2784099 ],\n",
      "       [-0.25238176,  0.44791167,  0.04542042, ..., -0.08911518,\n",
      "         0.02283473, -0.47249889],\n",
      "       ...,\n",
      "       [-0.12842189,  0.34854331, -0.39692622, ...,  0.45138558,\n",
      "        -0.39424573,  0.3634348 ],\n",
      "       [-0.2148877 ,  0.2059225 , -0.08653737, ..., -0.21278756,\n",
      "        -0.44101672,  0.34541789],\n",
      "       [ 0.36219095,  0.09320487, -0.35586029, ...,  0.07260748,\n",
      "        -0.11117414,  0.4739996 ]]), array([[ 0.36526489],\n",
      "       [ 0.12821742],\n",
      "       [ 0.24840294],\n",
      "       [ 0.41203983],\n",
      "       [ 0.47397289],\n",
      "       [-0.19598949],\n",
      "       [ 0.1262794 ],\n",
      "       [ 0.02418907],\n",
      "       [ 0.70758139],\n",
      "       [ 0.6741157 ],\n",
      "       [-0.09679425],\n",
      "       [ 0.54313987],\n",
      "       [ 0.66283465],\n",
      "       [-0.15852099],\n",
      "       [ 0.31991898],\n",
      "       [ 0.22445462],\n",
      "       [ 0.28103604],\n",
      "       [ 0.12557614],\n",
      "       [ 0.7413857 ],\n",
      "       [-0.04807403]])), (W2, b2) = (array([[ 5.05565722e-01, -5.88267396e-02, -3.22845286e-01,\n",
      "         5.10015410e-02,  2.78546890e-01, -4.81301091e-02,\n",
      "         3.44852315e-01, -1.31689184e-01,  4.53682614e-01,\n",
      "        -4.61604144e-01, -4.31141515e-01,  9.23530717e-02,\n",
      "        -1.33453881e-01,  3.33932431e-01,  4.19884419e-01,\n",
      "         2.81586395e-01, -2.53059304e-01,  3.09742059e-01,\n",
      "        -1.99713060e-01,  4.95149446e-01],\n",
      "       [ 3.75340376e-01, -1.78233061e-01,  3.15421661e-01,\n",
      "        -1.14406281e-01, -8.66202360e-02,  1.22027925e-02,\n",
      "        -3.51286462e-01, -1.37673378e-01,  4.33992534e-01,\n",
      "        -3.00839220e-01, -2.58850409e-01, -2.44824734e-01,\n",
      "        -2.10823292e-01, -4.42206052e-01, -3.14211599e-01,\n",
      "         2.15075252e-01,  2.15429369e-01, -2.80118100e-02,\n",
      "        -2.19842573e-01,  5.02602713e-01],\n",
      "       [ 4.66944163e-02,  1.17298512e-01,  2.69807169e-03,\n",
      "        -1.13948647e-01, -3.29250762e-01,  5.00260820e-01,\n",
      "        -4.71594310e-01,  1.13600934e-01, -8.21033251e-02,\n",
      "         1.57084428e-01,  1.92424880e-01,  1.72607074e-01,\n",
      "         4.10033642e-01, -3.20326559e-01, -5.02756560e-02,\n",
      "         1.61935274e-01,  3.13792708e-01, -1.99270796e-01,\n",
      "         8.50885978e-02, -3.68812401e-01],\n",
      "       [ 6.66210057e-02,  3.37271713e-02,  1.25064829e-02,\n",
      "         2.00017055e-01,  6.49641932e-01, -2.61064839e-01,\n",
      "         8.02626028e-02,  1.79514513e-01, -1.75026273e-01,\n",
      "        -3.31474202e-01, -4.81875173e-01,  2.21051342e-01,\n",
      "         4.02517318e-01,  3.67929445e-01,  4.41042212e-01,\n",
      "        -3.65131649e-01, -2.22896477e-01, -3.90941447e-02,\n",
      "        -3.60343769e-01,  3.95394826e-01],\n",
      "       [-2.17299238e-01, -3.36739799e-01, -2.62460861e-01,\n",
      "        -3.37766226e-01,  3.88313101e-01,  5.34770586e-01,\n",
      "         2.14427064e-01,  3.10832677e-01,  1.32333999e-01,\n",
      "        -3.74126859e-01, -1.78018843e-01, -3.76895181e-01,\n",
      "        -4.84027145e-01,  9.97343183e-02,  4.10537439e-01,\n",
      "         2.19692043e-01,  3.23743299e-01,  2.22576954e-01,\n",
      "         4.54386905e-01, -1.99844447e-02],\n",
      "       [-3.62169930e-01,  7.29583726e-02, -9.83927005e-02,\n",
      "        -2.24443231e-01, -3.30390010e-01,  2.15077545e-01,\n",
      "        -3.68062531e-01,  2.27032837e-01, -1.96156457e-01,\n",
      "         5.85558869e-01, -3.47743624e-01, -1.21958389e-01,\n",
      "         4.93149230e-01, -9.10313892e-02, -1.22630032e-01,\n",
      "         3.16756633e-01, -1.41039852e-01, -3.35650479e-01,\n",
      "        -2.56933075e-01,  9.83430319e-02],\n",
      "       [-1.48167818e-01,  1.41907578e-01,  1.39135733e-01,\n",
      "         1.84287142e-01,  3.28828579e-01, -4.57362934e-01,\n",
      "        -2.93225875e-01, -6.84086516e-02, -2.04663651e-01,\n",
      "         3.08366175e-01, -5.38446359e-01, -2.32927693e-01,\n",
      "         4.51960954e-02,  5.06484570e-01, -1.74741914e-01,\n",
      "         2.08519616e-01, -3.84351803e-01,  8.41329181e-02,\n",
      "         7.52387973e-02,  1.69574405e-01],\n",
      "       [ 2.43593254e-01, -4.04250988e-01,  1.46115665e-01,\n",
      "         4.06576136e-01,  4.23479841e-02, -1.50568236e-03,\n",
      "        -5.57933243e-01,  1.31548043e-01,  4.86152008e-01,\n",
      "         4.23751325e-01, -2.36448469e-01, -3.60129363e-01,\n",
      "         2.59730425e-01,  3.70129641e-01, -8.46313455e-02,\n",
      "        -2.76064309e-01,  3.22297668e-01, -2.83220535e-01,\n",
      "         5.90917298e-02, -2.79894486e-01],\n",
      "       [-4.40011158e-01,  4.56471105e-02,  2.20482816e-01,\n",
      "         5.43977409e-02, -2.73376929e-01,  1.48997220e-01,\n",
      "         5.29523411e-01, -1.19180838e-01, -9.79095129e-02,\n",
      "         2.14489739e-01, -2.93069556e-01, -3.06231466e-01,\n",
      "        -2.22492194e-01, -3.70778404e-02,  2.21269518e-01,\n",
      "        -3.99919530e-01,  3.63425103e-01, -3.99406450e-01,\n",
      "         4.68976227e-01,  2.42721467e-01],\n",
      "       [ 3.96931422e-01, -3.13078432e-01, -4.78574650e-01,\n",
      "         3.94007632e-01, -2.95004451e-01, -2.67861729e-01,\n",
      "         4.19522012e-01, -2.90191900e-01, -4.53940539e-01,\n",
      "         1.70560889e-01, -4.06526885e-02, -4.92857229e-01,\n",
      "         1.85151220e-01,  2.72928839e-01,  2.21973552e-01,\n",
      "        -4.74904417e-01,  3.45517790e-01,  3.62956261e-01,\n",
      "         4.01705250e-01, -2.05855448e-01],\n",
      "       [-2.15573620e-01, -4.82720997e-01,  2.66790894e-02,\n",
      "         3.66537979e-01,  3.82670512e-02, -1.82922217e-01,\n",
      "         2.85837749e-01, -2.87220189e-01,  4.22604241e-01,\n",
      "        -4.13318844e-01, -7.04086819e-02, -4.73875011e-01,\n",
      "        -2.25666996e-01, -2.48846312e-01,  1.44049934e-02,\n",
      "         1.95985698e-02, -2.89530650e-01,  8.87548089e-02,\n",
      "        -4.73859701e-01, -4.87992911e-02],\n",
      "       [ 1.27263995e-01,  4.23216522e-01, -2.30186864e-01,\n",
      "        -1.64161678e-01,  1.93576888e-01,  1.95434132e-01,\n",
      "        -5.97886049e-02, -4.38424775e-01,  7.53292391e-02,\n",
      "        -1.14759649e-01, -8.53716504e-06, -3.15718262e-01,\n",
      "         8.99220914e-02, -4.10088516e-01, -6.85026194e-02,\n",
      "         1.89379433e-01,  1.61280351e-01,  2.42449245e-01,\n",
      "         3.44634861e-01, -4.02582043e-01],\n",
      "       [-1.90594978e-01,  3.93596573e-01, -2.13117707e-01,\n",
      "        -1.79276124e-01, -2.72530034e-01, -3.95825318e-01,\n",
      "         3.87855218e-01,  4.12799117e-01,  3.33812265e-01,\n",
      "        -3.94836660e-02,  3.38923823e-01,  1.35332848e-01,\n",
      "        -3.33524697e-01,  3.22832087e-03,  3.97479489e-01,\n",
      "         4.34812866e-01,  3.50581809e-01,  7.36733742e-02,\n",
      "        -2.19422773e-01,  4.03440003e-01],\n",
      "       [ 3.11538630e-01, -2.92941770e-01,  2.86034188e-02,\n",
      "         5.01456793e-01,  1.85181216e-01, -4.00482668e-01,\n",
      "        -8.42617261e-02,  3.24980012e-03, -1.84625946e-01,\n",
      "         4.84008710e-01, -8.62479437e-02, -3.32806136e-01,\n",
      "        -6.14213568e-02,  1.91189521e-01, -9.19713919e-02,\n",
      "         1.52503111e-02, -2.78584819e-01, -3.99601915e-01,\n",
      "         2.96454494e-01, -8.32743550e-02],\n",
      "       [-3.87733454e-01,  1.89940805e-01,  1.79734543e-01,\n",
      "         2.62576419e-01,  6.78492871e-03, -3.37696052e-01,\n",
      "         4.22103979e-01, -1.46665758e-01, -2.56243526e-01,\n",
      "        -1.91718495e-01,  9.94495777e-02,  3.53834248e-01,\n",
      "        -6.25120198e-02, -2.97732543e-02, -3.88309807e-01,\n",
      "        -3.99136242e-01, -2.34469548e-01, -2.33940844e-01,\n",
      "         3.80365426e-01, -3.43237431e-02]]), array([[-0.2214928 ],\n",
      "       [-0.05842136],\n",
      "       [ 0.04393389],\n",
      "       [-0.47942068],\n",
      "       [ 0.21377691],\n",
      "       [-0.19415567],\n",
      "       [ 0.00528371],\n",
      "       [-0.48697772],\n",
      "       [-0.26289808],\n",
      "       [-0.22876266],\n",
      "       [-0.21060652],\n",
      "       [ 0.16217963],\n",
      "       [ 0.13564952],\n",
      "       [ 0.28233524],\n",
      "       [-0.09308757]])), (W3, b3) = (array([[-0.40961661,  0.30929877,  0.68209414, -0.47280299, -0.21582221,\n",
      "         0.37805854,  0.00173483, -0.12529934, -0.41618156, -0.29320992,\n",
      "        -0.16439108, -0.29664273,  0.1851605 , -0.39873893,  0.52141404],\n",
      "       [ 0.78569235,  0.59663479,  0.17854081, -0.38577207, -0.46997402,\n",
      "        -0.30793936, -0.26609409, -0.17761228, -0.55180126, -0.35435254,\n",
      "        -0.23011292, -0.48517104, -0.08543108, -0.31152115,  0.33624242],\n",
      "       [ 0.32423129, -0.18224218, -0.214236  ,  0.2935255 , -0.0449488 ,\n",
      "        -0.35623034, -0.40441236, -0.36239419,  0.45785109, -0.41404327,\n",
      "        -0.48813354, -0.23142356,  0.02728401, -0.36770534, -0.26694513],\n",
      "       [-0.52457437, -0.01346774, -0.2439328 ,  0.10781058, -0.04265707,\n",
      "        -0.0039436 ,  0.2010918 , -0.23743947, -0.06661912, -0.14515565,\n",
      "         0.32236002, -0.12635398,  0.5165322 ,  0.07644207,  0.33000295],\n",
      "       [-0.33466441,  0.12417685, -0.38910332,  0.29549164,  0.09410632,\n",
      "         0.10493462,  0.42352856, -0.10978379, -0.248932  ,  0.32323345,\n",
      "        -0.02094056, -0.07346376, -0.37389572,  0.26058518, -0.21627456],\n",
      "       [-0.47931868, -0.22416977, -0.07403478, -0.08712765, -0.12680588,\n",
      "         0.40297396, -0.35963989,  0.14657249,  0.04090538, -0.07339888,\n",
      "        -0.42502803, -0.06419455,  0.21992307,  0.09698811,  0.35098279],\n",
      "       [-0.06350196,  0.33131373,  0.12836679, -0.32029973,  0.38314369,\n",
      "        -0.33946739, -0.20046885,  0.13174538,  0.22574702,  0.62607327,\n",
      "         0.38812325,  0.11263072, -0.14562937, -0.55694917,  0.04317932],\n",
      "       [-0.12728948, -0.41742162,  0.365805  ,  0.12212992,  0.37139808,\n",
      "         0.43404643, -0.10495408,  0.55125587, -0.388196  , -0.2616537 ,\n",
      "         0.17165297, -0.2814918 , -0.30721441, -0.30192314,  0.35395333],\n",
      "       [ 0.05859498,  0.26720458,  0.1366268 , -0.27946571, -0.30222861,\n",
      "         0.10050458,  0.34077544, -0.26913151, -0.02524679, -0.38077617,\n",
      "        -0.29559723,  0.17425972,  0.19849861,  0.13964701, -0.0486576 ],\n",
      "       [ 0.09670144, -0.45102916, -0.29508865, -0.3227443 , -0.07152033,\n",
      "        -0.1142958 ,  0.40459407,  0.7542686 ,  0.02131826, -0.09854275,\n",
      "         0.02918323,  0.06239223, -0.67093837,  0.03034857,  0.15833522]]), array([[ 0.23374179],\n",
      "       [ 0.07003799],\n",
      "       [-0.22965264],\n",
      "       [-0.44980294],\n",
      "       [-0.21790416],\n",
      "       [ 0.43918693],\n",
      "       [ 0.38180818],\n",
      "       [-0.1812816 ],\n",
      "       [-0.0812369 ],\n",
      "       [ 0.35055553]]))\n"
     ]
    }
   ],
   "source": [
    "nn_cv = NeuralNetwork()\n",
    "W1, b1, W2, b2, W3, b3 = nn_cv.gradient_descent(X_cv, y_cv, 0.05, 1000)\n",
    "print(f\"Final weights and bias terms on cross-validation data: (W1, b1) = {W1, b1}, (W2, b2) = {W2, b2}, (W3, b3) = {W3, b3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's do a testing set next even though the CV set functioned as the test set earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The final accuracy is not exact, but it appears to be generally similar. The model likely has a slight overfitting issue (i.e: high variance), so an architecture with 1 less layer might be a better fit here. However, since this is a practice project, 3 layers were used just to get used to formally defining multiple hidden layers.\n",
    "\n",
    "#### Overall I'm pretty happy with how this turned out, and really excited to know that I can use this knowledge to debug models I make in the future using deep learning libraries!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
