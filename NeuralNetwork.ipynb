{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be making a neural network to solve a handwritten digit recognition problem from the MNIST data set.  No deep learning libraries such as Tensorflow and Pytorch will be used here. However, NumPy will be imported to increase efficiency and simulate the inner workings of deep learning libraries. Pandas and Matplotlib will also be used to help handle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read in the training data and get familiar with it. The training set here has 1 column describing the label (which digit the picture represents) followed by columns describing the color value of the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7   \n",
       "0      1       0       0       0       0       0       0       0       0  \\\n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779   \n",
       "0       0  ...         0         0         0         0         0         0  \\\n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the data into training and cross-validation (CV) sets. There are 42,000 examples so we will pick 10,000 random ones to use for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data before splitting: (42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(train_data)\n",
    "m, n = data.shape\n",
    "print(f\"Shape of data before splitting: {m, n}\")\n",
    "\n",
    "# Shuffle the data first\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# CV set\n",
    "cv = data[:10000].T\n",
    "X_cv = cv[1:n] / 255\n",
    "y_cv = cv[0]\n",
    "\n",
    "# Training set\n",
    "train = data[10000:m].T\n",
    "X_train = train[1:n] / 255\n",
    "y_train = train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's develop the Neural Network class we will be working with. This neural network will have 3 layers not including the input layer so I can get some practice developing multi-layered neural networks. The first layer will have 20 neurons, the second will have 15 neurons, and the last (output layer) will have 10 neurons corresponding to the 10 classes of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the neural network with random weights (w1, b1 , w2, b2, w3, b3)\n",
    "        '''\n",
    "\n",
    "        self.W1 = np.random.rand(20, 784) - 0.5\n",
    "        self.b1 = np.random.rand(20, 1) - 0.5\n",
    "        self.W2 = np.random.rand(15, 20) - 0.5\n",
    "        self.b2 = np.random.rand(15, 1) - 0.5\n",
    "        self.W3 = np.random.rand(10, 15) - 0.5\n",
    "        self.b3 = np.random.rand(10, 1) - 0.5\n",
    "    \n",
    "    def reLU(self, Z):\n",
    "        '''\n",
    "        Defines reLU activation function\n",
    "\n",
    "        Args:\n",
    "            Z (np.ndarray): Wn * An-1 + bn\n",
    "        \n",
    "        Returns:\n",
    "            reLU activation function applied to Z\n",
    "        '''\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def reLU_derivative(self, Z):\n",
    "        '''\n",
    "        Returns the derivative of reLU function\n",
    "\n",
    "        Args:\n",
    "            Z (np.ndarray): Wn * An-1 + bn\n",
    "        \n",
    "        Returns:\n",
    "            derivative of Z (1 or 0)\n",
    "        '''\n",
    "\n",
    "        return Z > 0\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        '''\n",
    "        Defines softmax activation function\n",
    "\n",
    "        Args:\n",
    "            Z (np.ndarray): Wn * An-1 + bn\n",
    "\n",
    "        Returns:\n",
    "            ndarray Z with softmax applied \n",
    "        '''\n",
    "\n",
    "        return np.exp(Z) / sum(np.exp(Z))\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        '''\n",
    "        Carry out forward propagation in neural network layers\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Training data\n",
    "        \n",
    "        Returns:\n",
    "            6-tuple containing intermediate terms (Z1, A1, Z2, A2, Z3, A3)\n",
    "        '''\n",
    "\n",
    "        Z1 = np.dot(self.W1, X) + self.b1\n",
    "        A1 = self.reLU(Z1)\n",
    "        Z2 = np.dot(self.W2, A1) + self.b2\n",
    "        A2 = self.reLU(Z2)\n",
    "        Z3 = np.dot(self.W3, A2) + self.b3\n",
    "        A3 = self.softmax(Z3)\n",
    "\n",
    "        return Z1, A1, Z2, A2, Z3, A3\n",
    "    \n",
    "    def one_hot(self, Y):\n",
    "        '''\n",
    "        Uses one-hot encoding to encode each label\n",
    "         \n",
    "        Args:\n",
    "            Y (np.ndarray): labels for data\n",
    "        \n",
    "        Returns:\n",
    "            one_hot (np.ndarray): one-hot encoded ndarray for labels of Y\n",
    "        '''\n",
    "\n",
    "        # Use max(Y) + 1 for # of columns because thats how many digits there are (0-9)\n",
    "        one_hot = np.zeros((Y.size, max(Y) + 1))\n",
    "        # For each row, apply one-hot encoding at the specified Y_row = Y column and turn it from 0 to 1\n",
    "        # Note: can do this iteratively, but numpy vectorizes this for us to make our NN a little more efficient\n",
    "        one_hot[np.arange(Y.size), Y] = 1\n",
    "        return one_hot.T\n",
    "    \n",
    "    # Note: some parameters of back prop are not used, but are included as parameters\n",
    "    # because it's a little easier to use since it follows the pattern Z, A, Z, A, Z, A, X, Y\n",
    "    # Slowdown because of this addition is negligible\n",
    "    def back_prop(self, Z1, A1, Z2, A2, Z3, A3, X, Y):\n",
    "        '''\n",
    "        Carry out backward propagation in neural netowrk layers\n",
    "\n",
    "        Args:\n",
    "            Z1 (np.ndarray): W1 * A0 + b1\n",
    "            A1 (np.ndarray): reLU(Z1)\n",
    "            Z2 (np.ndarray): W2 * A1 + b2\n",
    "            A2 (np.ndarray): reLU(Z2)\n",
    "            Z3 (np.ndarray): W3 * A2 + b3\n",
    "            A3 (np.ndarray): softmax(Z3)\n",
    "            X (np.ndarray): data\n",
    "            Y (np.ndarray): labels\n",
    "        \n",
    "        Returns:\n",
    "            6-tuple containing gradients (dW1, db1, dW2, db2, dW3, db3)\n",
    "        '''\n",
    "        \n",
    "        # Output layer\n",
    "        one_hot_y = self.one_hot(Y)\n",
    "        dZ3 = A3 - one_hot_y\n",
    "        dW3 = 1 / m * np.dot(dZ3, A2.T)\n",
    "        db3 = 1 / m * np.sum(dZ3)\n",
    "\n",
    "        # 2nd layer\n",
    "        dZ2 = np.dot(self.W3.T, dZ3) * self.reLU_derivative(Z2)\n",
    "        dW2 = 1 / m * np.dot(dZ2, A1.T)\n",
    "        db2 = 1 / m * np.sum(dZ2)\n",
    "\n",
    "        # 1st layer\n",
    "        dZ1 = np.dot(self.W2.T, dZ2) * self.reLU_derivative(Z1)\n",
    "        dW1 = 1 / m * np.dot(dZ1, X.T)\n",
    "        db1 = 1 / m * np.sum(dZ1)\n",
    "\n",
    "        return dW3, db3, dW2, db2, dW1, db1\n",
    "\n",
    "    def update(self, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
    "        '''\n",
    "        Updates weights and biases for gradient descent\n",
    "\n",
    "        Args:\n",
    "            dW1 (float32): gradient for layer 1 weight\n",
    "            db1 (float32): gradient for layer 1 bias\n",
    "            dW2 (float32): gradient for layer 2 weight\n",
    "            db2 (float32): gradient for layer 2 bias\n",
    "            dW3 (float32): gradient for layer 3 weight\n",
    "            db3 (float32): gradient for layer 3 bias\n",
    "            learning_rate (float32): learning rate alpha for gradient descent\n",
    "        '''\n",
    "\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n",
    "    \n",
    "    def gradient_descent(self, X_train, y_train, learning_rate, iters):\n",
    "        '''\n",
    "        Runs gradient descent to train the neural network\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): training data\n",
    "            y (np.ndarray): training labels\n",
    "            learning_rate (float32): learning rate alpha\n",
    "            iters (int32): number of iterations to run gradient descent\n",
    "        \n",
    "        Returns:\n",
    "            6-tuple containing trained weight and bias terms (W1, b1, W2, b2, W3, b3)\n",
    "        '''\n",
    "        \n",
    "        for i in range(1, iters + 1):\n",
    "            Z1, A1, Z2, A2, Z3, A3 = self.forward_prop(X_train)\n",
    "            dW3, db3, dW2, db2, dW1, db1 = self.back_prop(Z1, A1, Z2, A2, Z3, A3, X_train, y_train)\n",
    "            self.update(dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "\n",
    "            # Print status report every 100 iterations\n",
    "            if i % 100 == 0 or i == 1:\n",
    "                print(f\"Iteration: {i}\")\n",
    "                pred = np.argmax(A3, 0)\n",
    "                print(f\"Accuracy: {np.sum(pred == y_train) / y_train.size}\")\n",
    "        \n",
    "        return self.W1, self.b1, self.W2, self.b2, self.W3, self.b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot. Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Accuracy: 0.09084375\n",
      "Iteration: 100\n",
      "Accuracy: 0.4350625\n",
      "Iteration: 200\n",
      "Accuracy: 0.60384375\n",
      "Iteration: 300\n",
      "Accuracy: 0.68528125\n",
      "Iteration: 400\n",
      "Accuracy: 0.730125\n",
      "Iteration: 500\n",
      "Accuracy: 0.76246875\n",
      "Iteration: 600\n",
      "Accuracy: 0.782875\n",
      "Iteration: 700\n",
      "Accuracy: 0.800875\n",
      "Iteration: 800\n",
      "Accuracy: 0.81359375\n",
      "Iteration: 900\n",
      "Accuracy: 0.82590625\n",
      "Iteration: 1000\n",
      "Accuracy: 0.834875\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "W1, b1, W2, b2, W3, b3 = nn.gradient_descent(X_train, y_train, 0.05, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try it on the CV data to make sure we aren't overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Accuracy: 0.0916\n",
      "Iteration: 100\n",
      "Accuracy: 0.1779\n",
      "Iteration: 200\n",
      "Accuracy: 0.3049\n",
      "Iteration: 300\n",
      "Accuracy: 0.4038\n",
      "Iteration: 400\n",
      "Accuracy: 0.4929\n",
      "Iteration: 500\n",
      "Accuracy: 0.5616\n",
      "Iteration: 600\n",
      "Accuracy: 0.6133\n",
      "Iteration: 700\n",
      "Accuracy: 0.6561\n",
      "Iteration: 800\n",
      "Accuracy: 0.6835\n",
      "Iteration: 900\n",
      "Accuracy: 0.7096\n",
      "Iteration: 1000\n",
      "Accuracy: 0.7295\n"
     ]
    }
   ],
   "source": [
    "nn_cv = NeuralNetwork()\n",
    "W1, b1, W2, b2, W3, b3 = nn_cv.gradient_descent(X_cv, y_cv, 0.05, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The final accuracy is not exact, but it appears to be generally similar. The model likely has a slight overfitting issue (i.e: high variance), so an architecture with 1 less layer might be a better fit here. However, since this is a practice project, 3 layers were used just to get used to formally defining multiple hidden layers.\n",
    "\n",
    "#### Overall I'm pretty happy with how this turned out, and really excited to know that I can use this knowledge to debug models I make in the future using deep learning libraries!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
